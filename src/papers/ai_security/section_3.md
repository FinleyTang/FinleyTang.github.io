# 基于LLM大模型的漏洞检测分析

## LLM-Assisted Static Analysis for Detecting Security Vulnerabilities


### 链接
[arXiv:2405.17238](https://arxiv.org/pdf/2405.17238)

### 简要内容概况
本文提出了一种名为IRIS的新方法，它结合了大型语言模型（LLMs）和静态分析技术，以系统地进行全代码库推理，检测安全漏洞。IRIS利用了新创建的数据集CWE-Bench-Java，该数据集包含120个在真实Java项目中手动验证的安全漏洞。IRIS在检测漏洞方面表现出色，使用GPT-4模型时，检测到的漏洞数量是现有最先进的静态分析工具的两倍多，并且显著减少了误报数量。

### 重点内容解读
1. **背景**：软件安全漏洞对应用程序及其用户构成重大威胁。尽管静态污点分析等技术在检测漏洞方面取得了进展，但现有工具在实际应用中的有效性和可访问性受到限制。

2. **IRIS方法**：IRIS是首个将LLMs与静态分析结合的方法，用于检测安全漏洞。它通过LLMs挖掘特定于CWE（常见弱点枚举）的污点规范，并将这些规范与CodeQL等静态分析工具结合使用，以检测项目中的安全漏洞。

3. **数据集CWE-Bench-Java**：该数据集包含120个经过手动验证的安全漏洞，这些项目代码量庞大，平均超过30万行，最多达700万行。

4. **实验结果**：IRIS在使用GPT-4时检测到69个漏洞，而现有的静态分析工具仅检测到27个。此外，IRIS还显著减少了误报数量，最佳情况下减少了80%以上。

5. **关键技术点**：
   - **LLMs的应用**：IRIS利用LLMs的代码生成和推理能力，通过上下文信息和路径敏感信息来提高漏洞检测的准确性。
   - **静态分析工具的结合**：IRIS将LLMs的推理结果与CodeQL等工具结合，以执行更精确的全代码库分析。
   - **误报减少技术**：IRIS通过上下文分析技术，减少了静态分析中常见的误报问题。

### 小结
IRIS展示了LLMs与静态分析工具结合在软件安全漏洞检测中的潜力。通过使用大型语言模型来增强传统的静态分析方法，IRIS不仅提高了漏洞检测的覆盖率，还减少了误报，这对于提高软件安全性具有重要意义。未来的工作可能会探索这两种技术的更深层次整合，以进一步提高检测性能。


## Finetuning Large Language Models for Vulnerability Detection

### 链接
[arXiv:2401.17010](https://arxiv.org/pdf/2401.17010)

### 简要内容概况
本文介绍了对大型语言模型（LLMs）进行微调（finetuning），以用于源代码中的漏洞检测任务。研究团队采用了WizardCoder模型，这是StarCoder模型的一个改进版本，通过进一步的微调来适应漏洞检测任务。文章还探讨了不同的技术手段来提高对不平衡数据集的分类性能，并提出了加速训练过程的方法。微调后的WizardCoder模型在平衡和不平衡的漏洞数据集上的性能均优于类似CodeBERT的模型，证明了预训练的大型语言模型适应源代码漏洞检测任务的潜力。

### 重点内容解读
1. **模型选择与微调**：选择了WizardCoder作为基础模型进行微调，该模型在预训练阶段使用了大量代码语料库，具有强大的代码理解能力。
2. **训练过程优化**：为了加快训练速度，研究者修改了WizardCoder的训练过程，并探索了最优的训练制度。
3. **处理类别不平衡**：针对存在大量负样本和较少正样本的不平衡数据集，研究了不同的技术手段来提升分类性能。
4. **关键技术点**：
   - **PEFT（Parameter-Efficient Fine-Tuning）**：使用LoRA（Low-Rank Adaptation）方法对模型进行参数有效的微调，减少了训练参数和内存需求。
   - **Batch Packing**：为了解决短序列长度导致训练速度慢的问题，提出了将多个函数打包到每个训练序列中的技术，有效提高了计算效率。
   - **分类损失函数**：针对分类任务，提出了只使用最终token的预测概率与真实标签进行匹配的分类损失函数，而非传统的下一个token预测损失。
5. **实验结果**：微调后的WizardCoder模型在ROC AUC和F1指标上均优于ContraBERT和CodeBERT模型。

### 小结
本文通过微调大型语言模型WizardCoder，成功提升了源代码漏洞检测的性能。研究展示了通过技术优化和针对不平衡数据集的处理策略，可以显著提高模型的检测效果。此外，文章还提供了完整的可复现性包，为后续研究提供了便利。尽管取得了进步，但文章也指出了当前方法的局限性，并提出了未来可能的研究方向，如课程学习、主动采样和数据增强等技术，以更好地利用稀缺的少数类数据。

注：
> 在机器学习和特别是在安全漏洞检测的背景下，我们的目标是让模型能够准确地识别出所有类型的实例，不仅仅是“坏的”例子，即存在漏洞的代码片段，也包括“好的”例子，即没有漏洞的代码片段。以下是为什么需要识别两者的几个原因：
准确性：一个优秀的漏洞检测模型不仅需要能够识别出有漏洞的代码，还需要能够正确地识别出没有漏洞的代码。如果模型只关注于找出坏的例子，它可能会错误地将一些实际上没有漏洞的代码标记为有漏洞，这种类型的错误被称为“假阳性”。
可靠性：在安全领域，我们希望模型的判断是可靠的。如果一个模型频繁地产生假阳性，它可能会被用户不信任，因为它导致了很多不必要的关注和修复工作。

## GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis

### 网页链接
[https://arxiv.org/pdf/2308.03314](https://arxiv.org/pdf/2308.03314)

### 简要内容概况
本文介绍了GPTScan，这是一个结合了生成预训练变换器（GPT）和静态分析工具的新型工具，用于检测智能合约中的逻辑漏洞。智能合约由于其安全性问题，容易受到各种漏洞的攻击，导致巨额的经济损失。现有的分析工具通常只针对具有固定控制或数据流模式的漏洞，例如可重入和整数溢出。然而，这些工具无法有效检测与智能合约业务逻辑相关的漏洞。GPTScan通过将每种逻辑漏洞类型分解为场景和属性，利用GPT作为代码理解工具，匹配候选漏洞，并通过静态确认进一步验证，从而提高检测的准确性。

### 重点内容解读
1. **关键技术点**：GPTScan的核心在于它如何将GPT与静态分析结合起来。它不是简单地依赖GPT来识别漏洞，而是将GPT作为一种代码理解工具，通过识别关键变量和语句，然后通过静态确认模块进行验证。

2. **漏洞检测方法**：GPTScan将逻辑漏洞类型分解为代码级场景和属性。场景描述了可能发生逻辑漏洞的代码功能，而属性解释了易受攻击的代码属性或操作。

3. **多维过滤过程**：为了有效地缩小GPT匹配的候选函数范围，GPTScan采用了多维过滤过程。

4. **静态确认**：GPTScan使用静态确认来进一步验证GPT识别的关键变量和语句，通过数据流跟踪和符号执行等方法来确认漏洞的存在。

5. **性能评估**：在包含大约400个合约项目和3000个Solidity文件的数据集上进行评估，GPTScan显示出高精确度（超过90%）和可接受的精确度（57.14%），以及超过70%的召回率。

6. **成本效益**：GPTScan在扫描每千行Solidity代码时平均耗时14.39秒，成本为0.01美元，显示出快速且成本效益高的特点。

### 小结
GPTScan作为一个创新的工具，有效地结合了GPT的代码理解能力和静态分析的精确性，以检测智能合约中的逻辑漏洞。它通过细致的场景和属性匹配，以及后续的静态确认，提高了漏洞检测的准确性和效率。此外，GPTScan在实际数据集上的表现证明了其在大规模合约扫描中的实用性和成本效益。这项工作为智能合约安全性分析领域提供了新的视角和工具，有助于减少由于逻辑漏洞导致的财务损失。



## Detecting Insecure Code with LLMs

### 链接
[Detecting Insecure Code with LLMs](https://towardsdatascience.com/detecting-insecure-code-with-llms-8b8ad923dd98)

### 简要内容概况
本文由Melanie Hart Buehler撰写，发表于"Towards Data Science"，主要探讨了如何利用大型语言模型（LLMs）来检测Python代码中的安全漏洞。文章首先介绍了使用LLMs进行代码安全性分类的潜力，然后通过一系列实验评估了GPT4在识别不安全代码方面的能力。文章还讨论了Common Weakness Enumeration (CWE)的概念，以及静态代码分析工具的局限性，并提出了使用LLMs作为改进代码安全性实践的新方法。

### 重点内容解读
1. **LLMs在代码安全检测中的应用**：研究表明，LLMs能够有效地对代码进行安全分类，解释弱点，并提出修正建议，这可能显著简化传统的静态扫描安全编码实践。

2. **CWE的定义和分类**：CWE是由MITRE组织发布的软件和硬件弱点的公共分类和描述。文章特别关注了CWE的第一组，涉及不受信任数据源的弱点处理。

3. **静态代码分析工具的局限性**：虽然这些工具在检测代码中的安全问题方面很有用，但它们存在资源密集、误报率高和后续跟进耗时等缺点。

4. **LLMs的先前研究**：先前的研究展示了LLMs在开发生命周期的各个阶段的潜力，包括安全代码补全、测试用例生成、漏洞或恶意代码检测和错误修复。

5. **实验方法和结果**：文章通过零样本（zero-shot）和少样本（few-shot）学习等不同的提示技术，对GPT4进行了一系列实验，以评估其识别不安全代码的能力。实验结果显示，通过适当的提示模板和策略，可以提高LLMs的准确性和F1分数。

6. **关键技术点**：
   - **零样本学习**：不提供示例，仅根据指令进行预测。
   - **少样本学习**：提供几个成功的例子，然后让模型对未见过的代码执行相同的操作。
   - **KNN Few-shot**：使用K最近邻算法选择与输入代码最相似的示例，以提高响应质量。
   - **代码修复**：如果发现CWE，请求提供修复后的代码版本。

### 小结
文章通过一系列实验展示了LLMs在检测代码安全漏洞方面的潜力，尤其是在结合适当的提示技术和策略时。尽管LLMs在某些情况下能够超越传统方法，但它们也容易受到误报的影响，并且对提示的结构和措辞非常敏感。未来的工作可能包括测试更小的模型、使用包含CWE标签的提示模板、测试更大更多样化的数据集，以及评估LLMs提出的代码修复的安全性和功能性。


## Large Language Model for Vulnerability Detection: Emerging Results and Future Directions

### 链接
[arXiv:2401.15468](https://arxiv.org/pdf/2401.15468)

### 简要内容概况
本文探讨了大型预训练语言模型（LLMs）在软件漏洞检测中的有效性。以往的学习方法依赖于中等规模的预训练模型或从头开始训练的较小神经网络。然而，LLMs在检测软件漏洞方面的性能尚未得到充分研究。本文通过实验，特别是针对两种最先进的LLMs：GPT-3.5和GPT-4，展示了它们在不同提示（prompts）下的表现。结果显示，GPT-3.5与之前最先进的漏洞检测方法性能相当，而GPT-4则持续超越了现有技术。

### 重点内容解读
1. **LLMs在漏洞检测中的应用**：文章主要研究了LLMs在安全领域，特别是漏洞检测任务中的性能，这是之前研究中未充分探索的领域。
2. **实验设计与结果**：通过设计不同的提示（prompts），研究者们发现，适当的提示可以显著提高LLMs的漏洞检测性能。特别是，GPT-4在准确率方面比CodeBERT提高了34.8%。
3. **提示（Prompts）的设计**：文章详细讨论了如何设计有效的提示，包括任务描述、角色描述、项目信息、CWE（Common Weakness Enumeration）示例以及训练集中的样本等，以提高模型性能。
4. **In-Context Learning (ICL)**：介绍了一种替代的学习方法，即在不更新模型参数的情况下，通过适当的提示向模型传授特定任务的知识。
5. **未来研究方向**：文章提出了未来研究的多个方向，包括本地化和专业化的LLMs解决方案、提高漏洞检测的精确度和鲁棒性、处理长尾分布的漏洞数据，以及与开发者建立信任和协同作用。

### 小结
文章的主要贡献在于展示了LLMs在软件漏洞检测中的潜力，并通过精心设计的提示显著提升了模型性能。此外，文章还指出了当前研究的局限性，并提出了未来研究的方向，特别是在提高模型的精确度、鲁棒性以及与开发者的互动方面。这些研究不仅有助于提高软件安全性，也推动了LLMs在软件工程领域的应用发展。

### 关键技术点解读
- **大型预训练语言模型（LLMs）**：如GPT-3.5和GPT-4，这些模型因其在多种任务中展现出的少样本学习能力而受到关注。
- **In-Context Learning (ICL)**：一种适用于大型模型的学习方法，通过在模型参数固定的情况下，使用提示来传递任务特定知识。
- **Prompt Engineering**：设计有效的提示是提高LLMs在特定任务中性能的关键，包括任务描述、角色描述等。
- **漏洞检测性能评估**：使用准确率、精确度、召回率和F1分数等指标来评估模型的漏洞检测能力。
- **未来研究方向**：包括本地化模型、提高精确度和鲁棒性、处理长尾分布问题，以及与开发者建立信任。
## Project Naptime: Evaluating Offensive Security Capabilities of Large Language Models

### 网页链接
[Project Naptime: Evaluating Offensive Security Capabilities of Large Language Models](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html)

### 简要内容概况
文章由Sergei Glazunov和Mark Brand撰写，发布于Google Project Zero团队的博客。Project Naptime是一个旨在评估大型语言模型（LLMs）在安全漏洞研究中的攻击性安全能力的项目。文章介绍了大型语言模型在识别和演示安全漏洞方面的潜力，并提出了一套指导原则以提高LLMs在漏洞发现中的性能。通过这些原则，Google的研究人员显著提高了CyberSecEval 2基准测试的性能。

### 重点内容解读
1. **LLMs在安全研究中的应用**：随着LLMs代码理解和通用推理能力的提高，研究人员探索了这些模型在识别和演示安全漏洞方面的潜力。

2. **CyberSecEval 2**：Meta发布的CyberSecEval 2是一个包含新LLM基准的测试，用于发现和利用内存安全问题。但现有模型在这些挑战上的表现并不理想。

3. **指导原则**：为了有效评估LLMs在漏洞发现中的表现，文章提出了一系列原则，包括提供推理空间、交互式环境、专业工具、完美验证和采样策略。

4. **Naptime框架**：Google开发了一个名为"Naptime"的框架，用于LLM辅助的漏洞研究，特别关注自动化变体分析。Naptime利用专门的架构增强LLM执行漏洞研究的能力。

5. **CyberSecEval 2基准测试**：Naptime框架与CyberSecEval 2集成，针对C和C++代码中的"高级内存破坏"和"缓冲区溢出"类别进行了评估。

6. **实验结果**：通过改进测试方法，Naptime在"缓冲区溢出"测试中达到了1.00的新最高分，在"高级内存破坏"测试中达到了0.76的分数。

7. **结论**：尽管LLMs在提供正确工具的情况下可以开始执行基本的漏洞研究，但要实现对安全研究人员日常工作的有意义影响，仍需要取得实质性进展。

### 小结
文章强调了LLMs在安全领域的潜力，并通过Project Naptime展示了如何通过一系列原则和工具提高它们在漏洞发现中的性能。尽管取得了显著进展，但研究人员认为在LLMs能够对安全研究产生实质性影响之前，仍有许多工作要做。这包括开发更具挑战性和现实性的基准测试，以及确保基准测试方法能够充分利用LLMs的能力。


## Prompt-Enhanced Software Vulnerability Detection Using ChatGPT

### 链接
[arXiv:2308.12697](https://arxiv.org/pdf/2308.12697)

### 简要内容概况
本文探讨了使用ChatGPT进行软件漏洞检测的性能。随着软件漏洞引起的经济和社会损失日益增加，自动化漏洞检测在软件开发和维护中变得至关重要。近期，大型语言模型（LLMs）如GPT因其惊人的智能而受到广泛关注。尽管已有研究考虑使用ChatGPT进行漏洞检测，但它们并没有充分利用LLMs的特性，因为它们设计的问题过于简单，没有针对漏洞检测进行特定的提示设计。本文通过不同的提示设计，研究了使用ChatGPT进行软件漏洞检测的性能，并进行了大量实验来证明使用ChatGPT进行提示增强型漏洞检测的有效性。

### 重点内容解读
1. **背景与动机**：软件漏洞带来的风险日益严重，需要智能自动化方法进行检测。传统方法依赖规则和经典机器学习技术，存在局限性。

2. **ChatGPT与LLMs**：ChatGPT作为由LLM GPT驱动的聊天机器人，能够进行类人对话，用于生成文本、编程和回答问题等。

3. **提示工程（Prompt Engineering）**：通过设计文本提示来激发LLMs的潜力，这一技术在软件漏洞检测中尚未得到充分利用。

4. **研究方法**：本文通过应用各种改进到基本提示中，并结合结构化和顺序的辅助信息来改进提示设计，利用ChatGPT的多轮对话记忆能力设计适合漏洞检测的提示。

5. **实验与结果**：在两个漏洞数据集上进行了广泛的实验，证明了使用ChatGPT进行提示增强型漏洞检测的有效性，并对使用ChatGPT进行漏洞检测的优点和缺点进行了分析。

6. **关键技术点**：
   - **多轮对话记忆**：利用ChatGPT记住多步推理的能力，通过设计的思维链提示（chain-of-thought prompting），提高了漏洞检测的性能。
   - **辅助信息融合**：将API调用序列和数据流图（DFG）作为辅助信息融入提示中，帮助ChatGPT更好地理解代码和检测漏洞。
   - **提示设计范式**：研究了不同提示设计范式对于提高ChatGPT在不同编程语言上检测漏洞的有效性。

### 小结
本文通过精心设计的提示，显著提升了ChatGPT在软件漏洞检测任务中的性能。研究表明，结合特定的提示设计和辅助信息，LLMs能够在自动化软件安全领域发挥重要作用。同时，研究也指出了当前方法的局限性，并对未来的研究方向提出了建议。


