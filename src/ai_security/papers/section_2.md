### A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems


### 链接
[https://arxiv.org/pdf/2402.18649](https://arxiv.org/pdf/2402.18649)

### 简要内容概况
本文由Fangzhou Wu、Ning Zhang、Somesh Jha、Patrick McDaniel和Chaowei Xiao撰写，来自威斯康星大学麦迪逊分校和华盛顿大学圣路易斯分校。文章探讨了大型语言模型（LLM）系统的安全性问题。LLM系统由于其组成性质，除了核心的LLM外，还包括插件、沙盒等其他对象层。文章指出，尽管LLM系统具有巨大的潜力，但其安全性也引起了越来越多的关注。现有的LLM安全性研究通常集中在单独的LLM上，而没有考虑LLM与其他对象（如前端、Web工具、沙盒等）结合的生态系统。本文系统地分析了LLM系统的安全性，并提出了一种基于信息流的方法来形式化LLM系统的安全性。

### 重点内容解读
1. **多层安全分析**：文章提出了一种多层和多步骤的方法，应用于OpenAI GPT4，揭露了多个安全问题，这些问题不仅存在于LLM模型本身，还存在于其与其他组件的集成中。
2. **约束的存在与鲁棒性分析**：研究了LLM系统中约束的存在性以及这些约束的鲁棒性，特别是在对抗性环境中的表现。
3. **实际攻击场景**：构建了一个端到端的攻击，展示了攻击者如何在不操纵用户输入或直接访问OpenAI GPT4的情况下，非法获取用户的聊天记录。

### 小结
文章强调了LLM系统安全性的重要性，并提出了一种新的分析框架来系统地研究这些问题。研究发现，即使OpenAI GPT4设计了众多安全约束来提高其安全性特性，这些约束仍然容易受到攻击者的攻击。文章还提出了一些潜在的缓解策略，旨在提高LLM系统的鲁棒性，并从系统的角度而不是仅仅关注单独模型来解决问题。

### 关键技术点解读
- **信息流分析**：将安全性问题视为信息流对齐的约束，这是理解LLM系统安全性的基础。
- **概率性质**：LLM系统的输出具有不确定性，这对安全特性的一致性和可预测性构成了挑战。
- **多步骤分析方法**：通过多步骤过程来识别和评估LLM系统中的安全问题，包括识别必要的约束、验证现有约束以及评估约束的鲁棒性。
- **端到端攻击**：展示了攻击者如何利用发现的漏洞进行实际攻击，包括绕过安全检查和隐蔽地获取用户数据。

文章的发现对于理解和改进LLM系统的安全性具有重要意义，并为未来的研究和实践提供了宝贵的见解。